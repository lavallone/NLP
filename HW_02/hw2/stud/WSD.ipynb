{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S7pAk0A9TK70"
      },
      "source": [
        "# **Homework 2 - Word Sense Disambiguation** "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HO8uLXP5Wf4O"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LKij-njNdvYg"
      },
      "outputs": [],
      "source": [
        "# import stuffs\n",
        "from src.data_module import WSD_DataModule\n",
        "from src.hyperparameters import Hparams\n",
        "from src.train import train_model\n",
        "from src.model import WSD_Model\n",
        "from src.utils import three_group_bar, plot_histogram, evaluation_pipeline\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import Counter\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import wandb\n",
        "from dataclasses import asdict\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# to have a better workflow using python notebooks\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BiP2Q7ixdwuX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 1999\n"
          ]
        }
      ],
      "source": [
        "# setting the seed\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    _ = pl.seed_everything(seed)\n",
        "set_seed(1999)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Look at the data! üëÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams = asdict(Hparams())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# total number of senses\n",
        "d = json.load(open(hparams[\"prefix_path\"]+hparams[\"sense_map\"], \"r\"))\n",
        "all_senses_list = list(d.keys()) # 2158\n",
        "\n",
        "all_senses_train = []\n",
        "for batch in tqdm(data.train_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        for candidate in candidate_set:\n",
        "            all_senses_train.append(candidate)            \n",
        "c_train = Counter(all_senses_train)\n",
        "print(f\"total senses for train dataset: {len(c_train)}\") # 2092 \n",
        "\n",
        "all_senses_val = []\n",
        "for batch in tqdm(data.val_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        for candidate in candidate_set:\n",
        "            all_senses_val.append(candidate)\n",
        "c_val = Counter(all_senses_val)\n",
        "print(f\"total senses for val dataset: {len(c_val)}\") # 750\n",
        "            \n",
        "all_senses_test = []\n",
        "for batch in tqdm(data.test_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        for candidate in candidate_set:\n",
        "            all_senses_test.append(candidate)\n",
        "c_test = Counter(all_senses_test)\n",
        "print(f\"total senses for test dataset: {len(c_test)}\") # 781\n",
        "\n",
        "# check that all the senses of train/val/test sets are not lost and that are present in all_senses_list\n",
        "i=0\n",
        "print(\"train senses not present:\")  \n",
        "for s in list(c_train.keys()):\n",
        "    if s not in all_senses_list:\n",
        "        i+=1\n",
        "print(i)\n",
        "\n",
        "i=0\n",
        "print(\"val senses not present:\")  \n",
        "for s in list(c_val.keys()):\n",
        "    if s not in all_senses_list:\n",
        "        i+=1\n",
        "print(i)\n",
        "\n",
        "i=0\n",
        "print(\"test senses not present:\")    \n",
        "for s in list(c_test.keys()):\n",
        "    if s not in all_senses_list:\n",
        "        i+=1\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's build sense2id and id2sense map for coarse-graned senses\n",
        "sense2id = {}\n",
        "id2sense = {}\n",
        "\n",
        "idx=0\n",
        "for sense in all_senses_list:\n",
        "    sense2id[sense] = idx\n",
        "    id2sense[idx] = sense\n",
        "    idx+=1\n",
        "\n",
        "sense2id[\"<UNK>\"] = idx\n",
        "id2sense[idx] = \"<UNK>\"\n",
        "    \n",
        "json.dump(sense2id, open(hparams[\"prefix_path\"]+\"model/files/\"+hparams[\"coarse_or_fine\"]+\"_sense2id.json\", \"w\"))\n",
        "json.dump(id2sense, open(hparams[\"prefix_path\"]+\"model/files/\"+hparams[\"coarse_or_fine\"]+\"_id2sense.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's build sense2id and id2sense map for fine-graned senses\n",
        "d = json.load(open(hparams[\"prefix_path\"]+hparams[\"sense_map\"], \"r\"))\n",
        "all_senses_list = []\n",
        "for k in d.keys(): # 2158\n",
        "    for fine_s in d[k]:\n",
        "        all_senses_list.append(list(fine_s.keys())[0])\n",
        "\n",
        "sense2id = {}\n",
        "id2sense = {}\n",
        "\n",
        "idx=0\n",
        "for sense in all_senses_list:\n",
        "    sense2id[sense] = idx\n",
        "    id2sense[idx] = sense\n",
        "    idx+=1\n",
        "\n",
        "sense2id[\"<UNK>\"] = idx\n",
        "id2sense[idx] = \"<UNK>\"\n",
        "    \n",
        "json.dump(sense2id, open(hparams[\"prefix_path\"]+\"model/files/\"+hparams[\"coarse_or_fine\"]+\"_sense2id.json\", \"w\"))\n",
        "json.dump(id2sense, open(hparams[\"prefix_path\"]+\"model/files/\"+hparams[\"coarse_or_fine\"]+\"_id2sense.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sentence length histogram\n",
        "from src.utils import read_dataset\n",
        "\n",
        "train_sentences, _ = read_dataset(hparams[\"prefix_path\"]+hparams[\"data_train\"])\n",
        "sent_lengths_list = []\n",
        "for item in train_sentences:\n",
        "    sent_lengths_list.append(len(item[\"words\"]))\n",
        "    \n",
        "plot_histogram(sent_lengths_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filtering the sentences\n",
        "from src.utils import read_dataset\n",
        "\n",
        "train_sentences, _ = read_dataset(hparams[\"prefix_path\"]+hparams[\"data_train\"])\n",
        "sent_lengths_list = []\n",
        "tot_sentences, filtered_sentences = 0, 0\n",
        "for item in train_sentences:\n",
        "    tot_sentences += 1\n",
        "    if len(item[\"words\"])< 5 or len(item[\"words\"])> 85:\n",
        "        filtered_sentences += 1\n",
        "        continue\n",
        "    sent_lengths_list.append(len(item[\"words\"]))\n",
        "plot_histogram(sent_lengths_list)\n",
        "print(\" ______________________________________________________________________________\")\n",
        "print(f\"|After the filtering we've lost only the {round((filtered_sentences/tot_sentences)*100,2)}% of  original training sentences!|\")\n",
        "print(\" ______________________________________________________________________________\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's see how we are dealing with <UNK> token and how to use or if it is needed a clean_token() function!\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "tot_tokens = 0\n",
        "tot_unk = 0\n",
        "for batch in tqdm(data.train_dataloader()):\n",
        "    for input in batch[\"input\"][\"input_ids\"]:\n",
        "        for e in input:\n",
        "            if e.item() == 0:\n",
        "                break\n",
        "            tot_tokens+=1\n",
        "            if e.item() == 100:\n",
        "                tot_unk+=1\n",
        "print(f\"we have a total of {tot_tokens} tokens\")\n",
        "print(f\"with {tot_unk} <UNK> tokens!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> It's unbelievable how, thanks to the **BERT Tokenizer** (*WordPiece*), we generate zero \\<UNK\\> tokens! "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VexyCqTojBxc"
      },
      "source": [
        "## Look at the data! üëÄ"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hGie34asxn36"
      },
      "source": [
        "The first thing to do before starting a new deep learning project is to look at the data! Without a quality dataset there's no way of achieving good results. In our case, the given dataset is quite *clean* and does not required much work but anyway there's always room for improvement.\n",
        "\n",
        "A first check that's needed is to look at the data distribution. In the sense that *train*, *val* and *test* sets must have the same labels distribution. Otherwise all the future experiments will be meaningless and ineffective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5xTNi4Yow5I"
      },
      "outputs": [],
      "source": [
        "hparams = Hparams()\n",
        "data_train = [json.loads(e) for e in open(hparams.data_train, \"r\")]\n",
        "data_val = [json.loads(e) for e in open(hparams.data_val, \"r\")]\n",
        "data_test = [json.loads(e) for e in open(hparams.data_test, \"r\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "TlpsodU-3vcv",
        "outputId": "b3aa5b52-8acd-44de-c80a-6741d6e0ead0"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# TRAIN\n",
        "c = Counter([e for sublist in [d[\"labels\"] for d in data_train] for e in sublist])\n",
        "l = list(c.values())\n",
        "tot = 0\n",
        "for v in l:\n",
        "    tot += int(v)\n",
        "for i in range(len(l)):\n",
        "    l[i] = (l[i]/tot)*100\n",
        "data.append(l)\n",
        "\n",
        "# VAL\n",
        "c = Counter([e for sublist in [d[\"labels\"] for d in data_val] for e in sublist])\n",
        "l = list(c.values())\n",
        "tot = 0\n",
        "for v in l:\n",
        "    tot += int(v)\n",
        "for i in range(len(l)):\n",
        "    l[i] = (l[i]/tot)*100\n",
        "data.append(l)\n",
        "\n",
        "# TEST\n",
        "c = Counter([e for sublist in [d[\"labels\"] for d in data_test] for e in sublist])\n",
        "l = list(c.values())\n",
        "tot = 0\n",
        "for v in l:\n",
        "    tot += int(v)\n",
        "for i in range(len(l)):\n",
        "    l[i] = (l[i]/tot)*100\n",
        "data.append(l)\n",
        "\n",
        "columns = list(c.keys())\n",
        "three_group_bar(columns, data, \"train/val/test Labels Distribution\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gttft-3J9mYK"
      },
      "source": [
        "As we can see above the three distribution are quite similar and is pretty evident how the \"*O*\" label is the most dominant one. Hence, we are dealing with an *unbalanced* dataset and in fact all the choices that I've made during the development path of the project were selected to address this problem. <br> The context setting is low and for each sentence there are no many words to help the model to predict the events. For this reason, in my opinion, has been crucial to first **clean the tokens** from any dirtiness and create new and smart ones and secondly to **build a vocabulary** by generating as few *OOV* words as possible because any word is precious in a task like this."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Va8qDh2_E9nz"
      },
      "source": [
        "### Clean tokens üßπ"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4RAc71hFFCe5"
      },
      "source": [
        "As I 've already said, the dataset wasn't looking too bad, but some *cleaning* operations were needed to achieve better results on the task. <br> The choices made at this step had two different aims: \n",
        "* simple *cleaning* of tokens from weird symbols, punctuations, UNICODE characters and so on;\n",
        "*  *definition* of new tokens with the hope of helping the model to make good predictions based on their position and context.\n",
        "\n",
        "> üî∏ The function I implemented is \"*clean_tokens*\" from the *data.py* file and below I'll described the most significant changes that I executed. Of course, this function is applied to all the dataset splits (*train/val/test*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6OxXVucKmSL"
      },
      "outputs": [],
      "source": [
        "# this is simply how you perform the cleaning\n",
        "train_sentences = [e[\"tokens\"] for e in data_train]\n",
        "clean_tokens(train_sentences)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "usKzBZW6K_Az"
      },
      "source": [
        "üóí Basic ***clean_tokens*** steps:\n",
        "*  in the dataset were often present this kind of misprint character sequence [\"<\", \"ref\", \"name=\", \"''\", \"AAIB_8/88\", \"''\", \">\"] that I decided to not consider;\n",
        "* for dealing with situations like the one above, I introduced a new token *\\<IGNORE\\>* hoping that the system would ignore it at prediction time. I also defined a set of punctuations/symbols to be substituted by this new token;\n",
        "* besides the fact of mapping the digit numbers from 0 to 10 to their string version (\"2\" ‚û° \"two\"), I introduced an other new token *\\<NUMBER\\>* for all the numbers that appeared in the dataset. This choice has been made for treating all the numbers at the same level of contextual importance;\n",
        "* a subset of punctuations/symbols that constituted a single token (the ones contained as a part of other words were simply deleted) were left untouched and considered as standalone tokens;\n",
        "* all the tokens were lower-cased, I deleted all the unicode characters (e.g. *\\u00f* ), I substituted all the types of brackets to the round ones and all this type of quotes \" ` \" to this one \" ' \". I replaced \"&\" with \"and\" and kept the saxon genitive \" 's \" as a single token. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO0jRXdhQGNS"
      },
      "source": [
        "### Build Vocabulary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hDJhlCuRQQls"
      },
      "source": [
        "Since we are going to leverage an architecture with the *embedding layer* at the beginning, we need to decide which **word embedding** to use. It's quite clear that by using it the performance of the model will increase. <br> *Word2Vec* was a possible choice but it generated, according to me, too many *OOV* words and *fastText* was maybe the best choice (many people tell so) but dealing with subwords embeddings was not the best idea. I therefore decided to use **GloVe**. I had three options from the Stanford website but at the end I decided to download \"***glove.6B.zip***\" because is *uncased*, it has a relatively small vocabulary size (*400K*) with respect to other options but big enough for my purposes and it has four different word embedding sizes (50, 100, 200 and 300) for making experiments. \n",
        "> *The **300** embedding size will perform better on my models!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL5I_Se1YxSv"
      },
      "outputs": [],
      "source": [
        "# download GloVe pretrained embeddings\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip ../../model/glove.6B.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ol9EK2vMYxjm"
      },
      "source": [
        "So the starting vocabulary will be the one implicitly defined by the GloVe files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3l96jkrZFik"
      },
      "outputs": [],
      "source": [
        "def build_glove_vocab(glove_file, emb_size=300):\n",
        "    word2id = OrderedDict()\n",
        "    id2emb = OrderedDict()\n",
        "    id = 0\n",
        "    for l in open(glove_file, \"r\"):\n",
        "        line = l.split(\" \")\n",
        "        word = line[0]\n",
        "        emb = np.asarray(line[1:], dtype=\"float64\")\n",
        "        word2id[word] = id\n",
        "        id2emb[id] = emb\n",
        "        id += 1\n",
        "    return word2id, id2emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp0w2qSGc5iu"
      },
      "outputs": [],
      "source": [
        "glove_file = \"model/glove/glove.6B.300d.txt\"\n",
        "word2id, id2emb = build_glove_vocab(glove_file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g0BwA_v4gFqk"
      },
      "source": [
        "> Since I introduced two new tokens ***\\<IGNORE\\>*** and ***\\<NUMBER\\>*** and I don't want to lose any information that can be used to benefit events prediction, I need to extend the vocabulary. I first have to select the most frequent words which are in my training set but not in the vocabulary and then add them to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzLRLvaXid9F",
        "outputId": "0142943f-9c29-4cd3-b699-f1841e296fd7"
      },
      "outputs": [],
      "source": [
        "# list of training OOV words\n",
        "oov_list = []\n",
        "for token in [e for sublist in [e[\"tokens\"] for e in data_train] for e in sublist]:\n",
        "    if token not in word2id and token not in oov_list:\n",
        "        oov_list.append(token)\n",
        "\n",
        "# count the frequencies of each OOV word\n",
        "c = Counter([e for sublist in [e[\"tokens\"] for e in data_train] for e in sublist if e in oov_list])\n",
        "# show the 100 most common ones\n",
        "c.most_common(100)\n",
        "\n",
        "# as a design choice I decide to pick only the train OOV words which have a frequency higher or equal than 10!\n",
        "words_to_add = []\n",
        "for e in c.most_common(100):\n",
        "    if e[1] >= 10:\n",
        "        words_to_add.append(e[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMOKt1nYe9Wc"
      },
      "outputs": [],
      "source": [
        "def build_extended_vocab(glove_file, words_to_add, emb_size=300):\n",
        "    word2id = OrderedDict()\n",
        "    id2emb = OrderedDict()\n",
        "    tot, numbers_tot = np.zeros(emb_size, dtype=\"float64\"), np.zeros(emb_size, dtype=\"float64\")\n",
        "    id, num = 0, 0\n",
        "    for l in open(glove_file, \"r\"):\n",
        "        line = l.split(\" \")\n",
        "        word = line[0]\n",
        "        emb = np.asarray(line[1:], dtype=\"float64\")\n",
        "        if word.isnumeric():\n",
        "            numbers_tot += emb\n",
        "            num += 1\n",
        "        word2id[word] = id\n",
        "        id2emb[id] = emb\n",
        "        tot += emb\n",
        "        id += 1\n",
        "    \n",
        "    for w in words_to_add:\n",
        "        word2id[w] = id\n",
        "        if w==\"<NUMBER>\":\n",
        "            emb = numbers_tot/num # average of all the numbers embeddings\n",
        "            id2emb[id] = emb\n",
        "            tot += emb\n",
        "        else:\n",
        "            emb = np.random.rand(emb_size) # random initialization of the embedding\n",
        "            id2emb[id] = emb\n",
        "            tot += emb\n",
        "        id += 1\n",
        "\n",
        "    word2id['<UNK>'] = id\n",
        "    id2emb[id] = tot/id # average of the other embeddings\n",
        "    word2id['<PAD>'] = id+1\n",
        "    id2emb[id+1] = np.zeros(emb_size, dtype=\"float64\")\n",
        "    return word2id, id2emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GicyyB7oXDn"
      },
      "outputs": [],
      "source": [
        "glove_file = \"model/glove/glove.6B.300d.txt\"\n",
        "word2id, id2emb = build_extended_vocab(glove_file, words_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm-USItkprC-"
      },
      "outputs": [],
      "source": [
        "# if not already there, you can save the vocabulary in \"model/files/vocabs\" as \"word2id.json\"\n",
        "json.dump(word2id, open(\"model/files/vocabs/word2id.json\", \"w\"))\n",
        "\n",
        "# if not already done, you can save the embedding layer in \"model/embeddings/300\" as \"embedding_layer.pth\"\n",
        "embedding_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(np.vstack(list(id2emb.values()))))\n",
        "torch.save(embedding_layer.state_dict(), \"model/embeddings/300/embedding_layer.pth\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xp6iTLsMrNmN"
      },
      "source": [
        "> üî∏ To check the goodness of my extended vocabulary and of the previous token cleaning process, I want to check if there are labeled *EVENTS* that are not present in the vocabulary (*OOV events*). If so, I'll intervene in some way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDOpUTbFsKOk"
      },
      "outputs": [],
      "source": [
        "for sample in data_train:\n",
        "    for i in range(len(sample[\"tokens\"])):\n",
        "        if sample[\"labels\"][i] != \"O\" and sample[\"tokens\"][i] not in word2id:\n",
        "            print(\"OOV event detected!\")\n",
        "# it doesn't print anything, so we're OK!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6dDXR1c5tJ39"
      },
      "source": [
        "> ‚ö° **Embeddings strategy**: given that the \"GloVe\" embeddings are already pretrained and instead the new words that I added to the extended version of it are randomly initialized, it made sense to me to differentiate their training behaviour. In fact, if the *hyperparameter* ***num_emb*** is set to 2, we have the option to stop the finetuning process of the pretrained GloVe embeddings after **stop_train_emb** epochs while  continuing the training of the \"added\" embeddings. For this purpose, in practice, we have to \"split\" the vocabulary and create two separate embedding layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KK0BmqBwWY-"
      },
      "outputs": [],
      "source": [
        "## GloVe embeddings\n",
        "id2emb_1 = {k:v for k,v in id2emb.items() if k<400000}\n",
        "# if not already done, you can save the embedding layer in \"model/embeddings/300\" as \"embedding_layer_1.pth\"\n",
        "embedding_layer_1 = torch.nn.Embedding.from_pretrained(torch.from_numpy(np.vstack(list(id2emb_1.values()))))\n",
        "torch.save(embedding_layer_1.state_dict(), \"model/embeddings/300/embedding_layer_1.pth\")\n",
        "\n",
        "## new added embeddings\n",
        "id2emb_2 = {str(int(k)-400000):v for k,v in id2emb.items() if k>=400000}\n",
        "# if not already done, you can save the embedding layer in \"model/embeddings/300\" as \"embedding_layer_2.pth\"\n",
        "embedding_layer_2 = torch.nn.Embedding.from_pretrained(torch.from_numpy(np.vstack(list(id2emb_2.values()))))\n",
        "torch.save(embedding_layer_2.state_dict(), \"model/embeddings/300/embedding_layer_2.pth\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9OTnM0GAzbdf"
      },
      "source": [
        "### Filter Sentences ü•Ö"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S-8umVyN0E7y"
      },
      "source": [
        "Another important step before finishing the preprocessing part, is to filter out the *training* sentences. This is something it has to be done only at training time because the test/val datasets don't have to be touched in this sense. <br> Let's first see which is the histogram of sentences length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "yDqHDQ-zzq7Q",
        "outputId": "1e559960-7df2-4c8e-9a80-5ce991e53488"
      },
      "outputs": [],
      "source": [
        "sent_lengths_list = [len(e) for e in train_sentences]\n",
        "plot_histogram(sent_lengths_list)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "__FksEIBLSN0"
      },
      "source": [
        "As we can see there are some \"sentence *outliers*\" which deviates from the majority of data and this is not good in machine learning because our aim is always to learn a data distribution. Actually the mean value is 25, but we have the maximum sentence length that reaches 343. This kind of training samples have to be avoided in order to not make the model learn other type of distribution. <br> Saying that, the first step to do is to filter sentences based on their length. I've choosen to set *min_sent_length=2* and *max_sent_length=60* as length bounds and they appeared to work pretty good. Other type of sentences that I wanted to delete from the train set, as not useful for the prediction process, were the ones with no \"*EVENTS*\", i.e. labels with all \"*O*\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "4ad7Kmvi2lNk",
        "outputId": "15e12e82-3cac-4203-f849-f001253f69f9"
      },
      "outputs": [],
      "source": [
        "train_sentences, train_labels = filter_sentences([e[\"tokens\"] for e in data_train], [e[\"labels\"] for e in data_train], word2id)\n",
        "sent_lengths_list = [len(e) for e in train_sentences]\n",
        "plot_histogram(sent_lengths_list)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuj4qddqPakI"
      },
      "source": [
        "> üî∏ Look now how good the shape is looking! Only *9%* of sentences are excluded (1778 sentences). The function I implemented to do this filtering is \"*filter_sentences*\" from the *data.py* file and is only applied to *training* data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5FcrCHdjQ9Jf"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZjrJeRTRBiw"
      },
      "source": [
        "The adviced model architecture is a simple composition of an **Embedding layer**, a **Sequence encoder** and at the end a **Classifier**. <br> What I've implemented for this homework:\n",
        "* *nn.Embedding* pytorch module to work as a lookup table with pretrained GloVe embeddings and additional embeddings. Possibility of split the embeddings to have two different ways of being trained.\n",
        "* *nn.LSTM* pytorch architecture was the immediate choice. I didn't try other *RNNs-like* architectures as *GRU* for example because honestly since the beginning I achieved good results and therefore I decided to focus on *hyperparameters tuning* (using *Sweep* feature of  WandB). Without *LSTM*, it would have been impossible to encode information about the context of words (basic for our task) and it's probably the main block of the model. With respect to *vanilla RNNs*, it overcomes the vanishing gradient problem and it most robust to long term dependencies between words. One of its greatest power is the possibility of encoding the sequence both from *left-to-right* and *from right-to-left* to further enhance contextualization (***Bi-LSTMS***). To increase the performance I played and tweaked with *hidden_dim*, *bidirectional* and *num_layers* hyperparameter values.\n",
        "* *nn.Linear* is the simplest classifier to use. I also implemented a *MLP classifier* with three layers and ReLu activation functions, but with poor results. Therefore I used the simplest version which performed better and has moreover less weights to train."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PofkCwLlazrG"
      },
      "source": [
        "### ‚ö° EXTRAS "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yZUPF523bda4"
      },
      "source": [
        "During the very first experiments I noticed that for the system was especially hard to predict correctly the \"*inside*\" tags. Reasoning about it, this can be due  to the scarcity of these type of labels. In fact, with  respect to the \"*begin*\" tags, they are really a lot fewer (see the first plot of labels distribution). But anyway, I approach this issue and tried to solve it. \n",
        "\n",
        "* The first idea was to sum to the word embeddings, as it happens in *Transformer* models, a **positional encoding**. In order to convey to the model a sort of knowledge about where the tokens are within the sequence. For example if the prediction is \"*B-ACTION*\" at position *i*, the system should understand that the probability that \"I-ACTION\" is at *i+1* is higher than position *i-1*. It didn't work as expected: maybe because the positional encoding expresses its full potential in the moment in which *attention* in involved. \n",
        "* The second idea was to leverage **POS tags**. This because I noticed that most of the time the *events* are verbs (VERB) and I thought that giving the model an information like this it would have helped it. However I think that giving POS tags is in general benefitting the overall system since it can learn or extrapolate underlying grammatical relations which I could never have noticed. *How to provide/combine them with the model?* I tried and thought many options but at the end the most intuitive one is the following: after computing the POS tags of the input sequence, I embed it in a simply *Embedding layer*, I then sum the result to the *LSTM output* and finally feed it to a *Linear layer* which learns to compute the best way (hopefully) of combining them. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d7QQ9HR6bU-Y"
      },
      "source": [
        "### Baseline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vImA1v13dwZh"
      },
      "source": [
        "<br>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th><center>hparams</center></th>\n",
        "    <th><center> </center></th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><center>load_pretrained_emb</center></td>\n",
        "    <td><center>False</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>hidden_dim</center></td>\n",
        "    <td><center>512</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>bidirectional</center></td>\n",
        "    <td><center>False</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>num_layers</center></td>\n",
        "    <td><center>1</center></td>\n",
        "  </tr>\n",
        "   <tr>\n",
        "    <td><center>dropout</center></td>\n",
        "    <td><center>0</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>‚ö° POS_emb</center></td>\n",
        "    <td><center>False</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>‚ö° positional_encode</center></td>\n",
        "    <td><center>False</center></td>\n",
        "  </tr>\n",
        " \n",
        "\n",
        "</table>\n",
        "<br>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "thuSXCHgcXcT"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BaynqYLrbhu9"
      },
      "source": [
        "## Training ‚õè"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_AzCKIquGvIC"
      },
      "source": [
        "> Throughout all the project I used ***WandB*** as logging tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jtRPKhY9jtY"
      },
      "outputs": [],
      "source": [
        "def training_pipeline(config=None):\n",
        "    hparams_tuning = False\n",
        "    version_name = \"deberta\"\n",
        "    with wandb.init(entity=\"lavallone\", project=\"NLP\", name=version_name, mode=\"online\", config=config):\n",
        "        seed = wandb.config.seed if hparams_tuning else 1999\n",
        "        set_seed(seed)\n",
        "        hparams = asdict(Hparams())\n",
        "        # when doing the hparams search, this is how each run we change them to search for the best combinations!\n",
        "        if hparams_tuning:\n",
        "            hparams[\"batch_size\"] = wandb.config.batch_size\n",
        "            hparams[\"dropout\"] = wandb.config.dropout\n",
        "            hparams[\"lr\"] = wandb.config.lr\n",
        "            hparams[\"hidden_dim\"] = wandb.config.hidden_dim\n",
        "            hparams[\"seed\"] = wandb.config.seed\n",
        "\n",
        "        data = WSD_DataModule(hparams)\n",
        "        model = WSD_Model(hparams)\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        model.to(device)\n",
        "        \n",
        "        train_model(data, model, experiment_name=version_name, patience=5, metric_to_monitor=\"val_micro_f1\", mode=\"max\", epochs=100, precision=hparams[\"precision\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1qHpDFmAEP6"
      },
      "outputs": [],
      "source": [
        "wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "training_pipeline()\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_SwNRcdPbjvg"
      },
      "source": [
        "### Hyperparameters tuning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q9sF5ldZO5g6"
      },
      "source": [
        "> *WandB* provides a very user-friendly and easy-to-use *automated hyperparameter search* tool called ***Sweep***. This is how I tuned my hyperparameters. It can be noticed that I wanted to search also for the *random seed generator* both for \"picking\" the most convenient network's weights initialization and for selecting the \"best\" training dataloaders combination when using the *mixing windows strategy*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCAV15D1bl14"
      },
      "outputs": [],
      "source": [
        "wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "sweep_config = {'method': 'random',\n",
        "                'metric': {'goal': 'maximize', 'name': 'val_macro_f1', 'target' : 0.73},\n",
        "                'parameters': {\n",
        "                                'batch_size': {'values': [64, 128, 256, 512]},\n",
        "                                'dropout': {'distribution': 'uniform', 'min': 0.3, 'max': 0.5},\n",
        "                                'emb_dim': {'values': [50, 100, 200, 300]},\n",
        "                                'lr': {'distribution': 'uniform', 'min': 1e-4, 'max': 1e-2},\n",
        "                                'num_layers': {'distribution': 'int_uniform', 'min': 1, 'max': 5},\n",
        "                                'hidden_dim': {'distribution': 'int_uniform', 'min': 200, 'max': 600},\n",
        "                                'seed': {'distribution': 'int_uniform', 'min': 1, 'max': 1000},\n",
        "                            }\n",
        "               }\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project=\"NLP\", entity=\"lavallone\")\n",
        "wandb.agent(sweep_id, function=training_pipeline, count=40)\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ue4txjgQRtGs"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SC-4QOv7LMm2"
      },
      "source": [
        "> I also implemented an *evaluation pipeline* to avoid to execute each time the provided *test.sh* shell script. In fact it uses *docker* üê≥, which consumes a lot of storage space each run. Of course I'll use it for the final evaluation of my best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wRe7SVyXMMJ_",
        "outputId": "1dddadae-d286-4b44-c048-578f22e5af02"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "best_ckpt = \"../../model/checkpoints/deberta-epoch=08-val_micro_f1=0.8740.ckpt\"\n",
        "model = WSD_Model.load_from_checkpoint(best_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "additional_infos = True\n",
        "if additional_infos:\n",
        "    classification_report = evaluation_pipeline(model, data, additional_infos=additional_infos)\n",
        "else:\n",
        "    evaluation_pipeline(model, data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
