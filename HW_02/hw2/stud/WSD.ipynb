{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S7pAk0A9TK70"
      },
      "source": [
        "# **Homework 2 - Word Sense Disambiguation** "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HO8uLXP5Wf4O"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LKij-njNdvYg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# import stuffs\n",
        "from src.data_module import WSD_DataModule, WSD_Gloss_DataModule\n",
        "from src.hyperparameters import Hparams\n",
        "from src.train import train_model\n",
        "from src.model import WSD_Model, WSD_Gloss_Model\n",
        "from src.utils import one_group_bar, plot_histogram, evaluation_pipeline\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import Counter\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "import wandb\n",
        "from dataclasses import asdict\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# to have a better workflow using python notebooks\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiP2Q7ixdwuX"
      },
      "outputs": [],
      "source": [
        "# setting the seed\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    _ = pl.seed_everything(seed)\n",
        "set_seed(1999)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Look at the data! 👀"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing to do before starting a new deep learning project is to look at the data! Without a quality dataset there's no way of achieving good results. In fact, a good *pre-processing* pipeline is a necessary condition for a high-quality **WSD system**. In the present case, the given dataset is quite *clean* and does not required much work but anyway there's always room for improvement.\n",
        "The significance of this aspect is frequently disregarded as WSD systems commonly depend on pre-parsed documents that have already been divided into sentences, tokenized, lemmatized, and POS-tagged (like in our dataset), but nothing should be taken for granted!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> In our approach to WSD, we employ a supervised method that involves training machine learning models on *sense-annotated* data. The first thing to do is indeed to create and define a **sense inventory**, that is the set of all possible senses for all the words involved in the task. The senses are *synsets* from ***WordNet*** and express the different meanings of a word. Therefore the engineering process of defining the correct candidate set of possible meanings remains central to the overall disambiguation process. Fortunately this work has already been done :)\n",
        "\n",
        "We are going to address two different WSD approaches:\n",
        "-   **fine-grained WSD**: the senses are the ones form WordNet and it aims to provide more precise and detailed sense distinctions.\n",
        "-   **coarse-grained WSD**: where we cluster the similar senses of a polysemous word to obtain a smaller number of sense distinctions. This approach aims to provide a higher-level understanding of word meanings, focusing on more general sense categories.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sense inventory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After making some reasonings, the easiest thing to do for build the sense invenctory was to take all the senses stored in the '*coarse_fine_defs_map.json*' file provided to us. We made all the possible checks to see if all the senses annotated in *train/val/test* datasets were not excluded and that's fortunately the case. \n",
        "\n",
        "> Of course the built sense inventory doesn't cover all the possible senses of all the possible polysemous words. Therefore I also implemented the logic for handle cases where the senses for the word to disambiguate are not present. In few words, by not cheating, if at least one sense among the candidate ones of the particular sample is \\<UNK\\>, the sense prediction will be <UNK> and consequently wrong.\n",
        "\n",
        "Saying that, there won't be these kind of problems when using *train/val/test* sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TOTAL NUMBER OF SENSES for coarse-grained WSD\n",
        "hparams = asdict(Hparams()) # instantiate hyperparamters file\n",
        "d = json.load(open(hparams[\"prefix_path\"]+hparams[\"sense_map\"], \"r\"))\n",
        "all_senses_list = list(d.keys()) # 2158\n",
        "print(f\"Length of sense inventory for coarse-grained WSD is {len(all_senses_list)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since we are dealing with neural networks we need to encode the sense invectory and simply create a mapping between \n",
        "# coarse-grained senses and indices.\n",
        "\n",
        "# let's build sense2id and id2sense map for coarse-grained senses\n",
        "sense2id = {}\n",
        "id2sense = {}\n",
        "\n",
        "idx=0\n",
        "for sense in all_senses_list:\n",
        "    sense2id[sense] = idx\n",
        "    id2sense[idx] = sense\n",
        "    idx+=1\n",
        "\n",
        "sense2id[\"<UNK>\"] = idx\n",
        "id2sense[idx] = \"<UNK>\"\n",
        "    \n",
        "json.dump(sense2id, open(hparams[\"prefix_path\"]+\"model/files/coarse_sense2id.json\", \"w\"))\n",
        "json.dump(id2sense, open(hparams[\"prefix_path\"]+\"model/files/coarse_id2sense.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "# TOTAL NUMBER OF TRAIN SENSES for coarse-grained WSD\n",
        "all_senses_train = []\n",
        "for batch in tqdm(data.train_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        for candidate in candidate_set:\n",
        "            all_senses_train.append(candidate)            \n",
        "c_train = Counter(all_senses_train)\n",
        "print(f\"Total senses for train dataset: {len(c_train)}\") # 1933\n",
        "\n",
        "# TOTAL NUMBER OF TRAIN SENSES for coarse-grained WSD\n",
        "all_senses_val = []\n",
        "for batch in tqdm(data.val_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        for candidate in candidate_set:\n",
        "            all_senses_val.append(candidate)\n",
        "c_val = Counter(all_senses_val)\n",
        "print(f\"Total senses for val dataset: {len(c_val)}\") # 750\n",
        "\n",
        "# TOTAL NUMBER OF TRAIN SENSES for coarse-grained WSD  \n",
        "all_senses_test = []\n",
        "for batch in tqdm(data.test_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        for candidate in candidate_set:\n",
        "            all_senses_test.append(candidate)\n",
        "c_test = Counter(all_senses_test)\n",
        "print(f\"Total senses for test dataset: {len(c_test)}\") # 781"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHECK that all senses present in train/val/test are included in the SENSE INVENTORY\n",
        "all_senses_list = [sense2id[e] for e in all_senses_list] # we encode the senses\n",
        "i=0 \n",
        "for s in list(c_train.keys()):\n",
        "    if s not in all_senses_list:\n",
        "        print(s)\n",
        "        i+=1\n",
        "assert i==0\n",
        "\n",
        "i=0 \n",
        "for s in list(c_val.keys()):\n",
        "    if s not in all_senses_list:\n",
        "        i+=1\n",
        "assert i==0\n",
        "\n",
        "i=0   \n",
        "for s in list(c_test.keys()):\n",
        "    if s not in all_senses_list:\n",
        "        i+=1\n",
        "assert i==0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> *Sense inventory* comprehend all our dataset coarse-grained senses! "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We do the same thing for **fine-grained** senses thanks to '*coarse_fine_defs_map.json*' file. The check we've done before it's now useless to perform since fine-grained senses are included in coarse-grained ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's build sense2id and id2sense map for fine-graned senses\n",
        "d = json.load(open(hparams[\"prefix_path\"]+hparams[\"sense_map\"], \"r\"))\n",
        "all_senses_list = [] # 4476\n",
        "for k in d.keys():\n",
        "    for fine_s in d[k]:\n",
        "        all_senses_list.append(list(fine_s.keys())[0])\n",
        "print(f\"Length of sense inventory for fine-grained WSD is {len(all_senses_list)}\")\n",
        "\n",
        "sense2id = {}\n",
        "id2sense = {}\n",
        "\n",
        "idx=0\n",
        "for sense in all_senses_list:\n",
        "    sense2id[sense] = idx\n",
        "    id2sense[idx] = sense\n",
        "    idx+=1\n",
        "\n",
        "sense2id[\"<UNK>\"] = idx\n",
        "id2sense[idx] = \"<UNK>\"\n",
        "    \n",
        "json.dump(sense2id, open(hparams[\"prefix_path\"]+\"model/files/fine_sense2id.json\", \"w\"))\n",
        "json.dump(id2sense, open(hparams[\"prefix_path\"]+\"model/files/fine_id2sense.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Because of some approaches I'll develop later I need \n",
        "# to build a direct mapping between fine and coarse-grained (we already have the opposite mapping)\n",
        "\n",
        "d = json.load(open(hparams[\"prefix_path\"]+hparams[\"sense_map\"], \"r\"))\n",
        "fine2coarse = {}\n",
        "for k in d.keys():\n",
        "    for fine_s in d[k]:\n",
        "        fine2coarse[list(fine_s.keys())[0]] = k\n",
        "\n",
        "json.dump(fine2coarse, open(hparams[\"prefix_path\"]+\"model/files/fine2coarse.json\", \"w\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Let's make a quick quantitative analysis for facing what the dataset and the task is about."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Number of candidates histogram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "num_candidates = []\n",
        "\n",
        "num_candidates_train = []\n",
        "for batch in tqdm(data.train_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        num_candidates_train.append(len(candidate_set))\n",
        "num_candidates.append(num_candidates_train)\n",
        "        \n",
        "num_candidates_val = []\n",
        "for batch in tqdm(data.val_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        num_candidates_val.append(len(candidate_set))\n",
        "num_candidates.append(num_candidates_val)\n",
        "        \n",
        "num_candidates_test = []\n",
        "for batch in tqdm(data.test_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        num_candidates_test.append(len(candidate_set))\n",
        "num_candidates.append(num_candidates_test)\n",
        "    \n",
        "plot_histogram(num_candidates, multiple=True, title=\"Number of Candidates Histogram\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We can say for sure that the three splits represent the same data distribution (and this is already a good starting point). It's quite obvious that most of polysemous words have the range of possible meanings between 1 and 5. The only one coarse-grained sense is most likely case and this, of course, will help to increase my models performance (in the fine-grained case this doesn't stand anymore). <br> But the thing that immediately stands out is the presence of words with more than 5 possible \"coarse\" meanings! In particular the 15 case!\n",
        "\n",
        "For this reason I'm going to save all candidate sets which have a length bigger than 5 for keeping track of my models performance on them (which should be the hardest cases!). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I want to compute and actually see which are the most difficult (theoretically) words to disambiguate\n",
        "# (an example is 'hall', 'find', 'stone', 'bound', 'bore', 'found' etc.)\n",
        "hard_words_to_disambiguate = []\n",
        "\n",
        "for batch in tqdm(data.train_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        if len(candidate_set) >= 5:\n",
        "            candidate_set = [id2sense[e] for e in candidate_set]\n",
        "            candidate_set.sort()\n",
        "            if (len(candidate_set), candidate_set) not in hard_words_to_disambiguate:\n",
        "                hard_words_to_disambiguate.append((len(candidate_set), candidate_set))\n",
        "for batch in tqdm(data.val_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        if len(candidate_set) >= 5:\n",
        "            candidate_set = [id2sense[e] for e in candidate_set]\n",
        "            candidate_set.sort()\n",
        "            if (len(candidate_set), candidate_set) not in hard_words_to_disambiguate:\n",
        "                hard_words_to_disambiguate.append((len(candidate_set), candidate_set))\n",
        "for batch in tqdm(data.test_dataloader()):\n",
        "    for candidate_set in batch[\"candidates\"]:\n",
        "        if len(candidate_set) >= 5:\n",
        "            candidate_set = [id2sense[e] for e in candidate_set]\n",
        "            candidate_set.sort()\n",
        "            if (len(candidate_set), candidate_set) not in hard_words_to_disambiguate:\n",
        "                hard_words_to_disambiguate.append((len(candidate_set), candidate_set))\n",
        "\n",
        "hard_words_to_disambiguate.sort(key=lambda x:x[0])\n",
        "print(f\"There are {len(hard_words_to_disambiguate)} polysemous words which are harder to disambiguate for their many meanings!\")\n",
        "print(hard_words_to_disambiguate)\n",
        "\n",
        "hard_words_to_disambiguate_dict = {}\n",
        "for i in range(len(hard_words_to_disambiguate)):\n",
        "    hard_words_to_disambiguate_dict[i] = hard_words_to_disambiguate[i][1]\n",
        "json.dump(hard_words_to_disambiguate_dict, open(hparams[\"prefix_path\"]+\"model/files/hard_words_to_disambiguate.json\", \"w\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Least frequent senses**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Another thing I want to see is which are the least frequent word senses in the *training* set. In this way I can understand and acknowledge which are the cases where my models will hypothetically suffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l = []\n",
        "for batch in tqdm(data.train_dataloader()):\n",
        "    for label in batch[\"labels\"]:\n",
        "        l.append(id2sense[label])\n",
        "c = Counter(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# just want to show the frequence histogram of each coarse-grained sense\n",
        "senses_frequence_list = list(c.values())\n",
        "plot_histogram(senses_frequence_list, title=\"Senses Frequence (train) Histogram\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> This is exactly the **Zipf distribution**: the reason why WSD is not a trivial task at all. It shows us that few senses occur very frequently while the majority of senses occur relatively infrequently! \n",
        "\n",
        "It doesn't make much sense to show the least training frequent senses because they are too many, but anyway I'm going to show for the sake of curiosity the least 70 frequent ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "least_common_senses = c.most_common(len(c))[-70:]\n",
        "columns = [e[0] for e in least_common_senses]\n",
        "data = [[e[1] for e in least_common_senses]]\n",
        "title = \"Least 70 frequent (train) senses\"\n",
        "one_group_bar(columns, data, title)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> As I've already mentioned the most difficult and time consuming part has been taken over by those who have made the dataset (collecting all candidate senses for each polysemous word and deciding which are the words worth the disambiguation). It would have been particularly boring for me to check if all sense synsets were coherent and there weren't any kind of inconsistency. Hence, holding a certain level of confidence in the dataset, I deem the examination I conducted on including all the senses within the sense inventory to be acceptable enough for this assignment."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> For this task, where we were allowed to use *Transformers*, all the pipeline part of building the vocabulary and all the concern about generating as few \\<UNK\\> tokens as possible does not exist anymore! \n",
        "\n",
        "Let's see why I'm telling this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's see how many <UNK> token we generate without any particular type of preprocessing!\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "tot_tokens = 0\n",
        "tot_unk = 0\n",
        "for batch in tqdm(data.train_dataloader()):\n",
        "    for input in batch[\"input\"][\"input_ids\"]:\n",
        "        for e in input:\n",
        "            if e.item() == 0: # we reached <PAD> tokens\n",
        "                break\n",
        "            tot_tokens+=1\n",
        "            if e.item() == 100: # is the <UNK> token\n",
        "                tot_unk+=1\n",
        "print(f\"We have a total of {tot_tokens} tokens\")\n",
        "print(f\"with {tot_unk} <UNK> tokens!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> It's unbelievable how, thanks to the **BERT Tokenizer** (*WordPiece*), we generate zero \\<UNK\\> tokens!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Clean tokens 🧹"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With respect to the first homework the *cleaning* operations (also due the power of *BERT Tokenizer*) are very basic and not \"aggressive\".\n",
        "\n",
        "> 🔸 The function I implemented is \"*clean_tokens*\" from the *data_module.py* file. Of course, this function is applied to all the dataset splits (*train/val/test*)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filter sentences 🥅"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another important step before finishing the preprocessing part, is to filter out the *training* sentences. This is something it has to be done only at training time because the test/val datasets don't have to be touched in this sense. <br> Let's first see which is the histogram of sentences length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sentence length histogram\n",
        "from src.utils import read_dataset\n",
        "train_sentences, _ = read_dataset(hparams[\"prefix_path\"]+hparams[\"data_train\"])\n",
        "sent_lengths_list = [len(item[\"words\"]) for item in train_sentences]\n",
        "    \n",
        "plot_histogram(sent_lengths_list)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see there are some \"sentence *outliers*\" which deviates from the majority of data and this is not good in machine learning because our aim is always to learn a data distribution. Actually the mean value is 40, but we have the maximum sentence length that reaches 289. This kind of training samples have to be avoided in order to not make the model learn other type of distribution. <br> Saying that, the first step to do is to filter sentences based on their length. I've choosen to set *min_sent_length=5* and *max_sent_length=85* as length bounds and they appeared to work pretty good. During the filtering I also check that in each sentence there's at least one word to disambiguate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filtering the sentences (the actual function is implemented in \"data_module.py\")\n",
        "from src.utils import read_dataset\n",
        "train_sentences, _ = read_dataset(hparams[\"prefix_path\"]+hparams[\"data_train\"])\n",
        "\n",
        "sent_lengths_list = []\n",
        "tot_sentences, filtered_sentences = 0, 0\n",
        "for item in train_sentences:\n",
        "    assert len(item[\"instance_ids\"].keys()) != 0\n",
        "    tot_sentences += 1\n",
        "    if len(item[\"words\"])< 5 or len(item[\"words\"])> 85:\n",
        "        filtered_sentences += 1\n",
        "        continue\n",
        "    sent_lengths_list.append(len(item[\"words\"]))\n",
        "\n",
        "plot_histogram(sent_lengths_list)\n",
        "print()\n",
        "print(f\"| After the filtering we've lost only the {round((filtered_sentences/tot_sentences)*100,2)}% of original training sentences! |\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuj4qddqPakI"
      },
      "source": [
        "> 🔸 Look now how good the shape is looking! Only *1.14%* of sentences are excluded (141 sentences). The function I implemented to do this filtering is \"*filter_sentences*\" from the *data_module.py* file and is only applied to *training* data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5FcrCHdjQ9Jf"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZjrJeRTRBiw"
      },
      "source": [
        "The most intuitive way of treating the problem is to see the WSD system as a ***Multiclass Token Classification***. There are some SOTA works which frame WSD as a *multi-label* classification problem, arguing that forcing a system to treat WSD as a *single-label* classification problem and\n",
        "learning that only one sense is correct for a word in a given context does not reflect how human beings disambiguate text (and I agree with this statement). But for the sake of simplicity I approached the task as a *single-label* problem.\n",
        "\n",
        "The model architecture is a \"simple\" composition of an **Embedding layer**, a **Transformer encoder** and at the end a **Classifier**. <br> \n",
        "*   **Bert**, **Roberta** and **Deberta** transformers architecture as ***FastTokenizers*** for the tokenization part and ***Encoders*** for the encoding part.\n",
        "*   **Classifier**: starting from the implementation of some papers produced by the *Sapienza NLP* group about WSD, I structured it in two layers. The first is **non-linear** and is anticipated by a batch normalization operation. As activation function I tried the strongly suggested *SiLu* and also *ReLu*. Here an important hyperparameter to tweak is the dimension in which transform the encoded text. The last layer is a simple **linear layer** which project the latent vector into the sense inventory space. \n",
        "\n",
        "The output of the WSD system provides a probability (thanks to a softmax) for each sense of the inventory. The model is trained to maximize the probability of the single most appropriate sense by minimizing a **cross-entropy** loss. At prediction time we focus on target senses only, which means we are going to choose only from the candidates set of the word under consideration."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Since we have the *pos tags* and *lemmas* \"for free\", I've also tried to involve them in the system. For example, using the lemmas of the words as input to the Transformers instead of the words themselves. Furthermore, I exploited the pos tags of the words to disambiguate by giving them as input to the Transformer encoder after input sentence ( *\\[CLS\\] input_sentence \\[SEP\\] pos_tag \\[SEP\\]* ). "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters configuration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d7QQ9HR6bU-Y"
      },
      "source": [
        "**Baseline**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vImA1v13dwZh"
      },
      "source": [
        "<br>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th><center>hparams</center></th>\n",
        "    <th><center> </center></th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><center>fine_tune_bert</center></td>\n",
        "    <td><center>False</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>encoder_type</center></td>\n",
        "    <td><center>bert</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>hidden_dim</center></td>\n",
        "    <td><center>256</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>act_fun</center></td>\n",
        "    <td><center>silu</center></td>\n",
        "  </tr>\n",
        "   <tr>\n",
        "    <td><center>dropout</center></td>\n",
        "    <td><center>0</center></td>\n",
        "  <tr>\n",
        "    <tr>\n",
        "    <td><center>⚡ precision</center></td>\n",
        "    <td><center>16</center></td>\n",
        "  </tr>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>⚡ use_POS</center></td>\n",
        "    <td><center>False</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>⚡ use_lemmas</center></td>\n",
        "    <td><center>False</center></td>\n",
        "  </tr>\n",
        " \n",
        "\n",
        "</table>\n",
        "<br>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Base model *(best configuration)***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th><center>hparams</center></th>\n",
        "    <th><center> </center></th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><center>fine_tune_bert</center></td>\n",
        "    <td><center>True</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>encoder_type</center></td>\n",
        "    <td><center>bert</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>hidden_dim</center></td>\n",
        "    <td><center>512</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>act_fun</center></td>\n",
        "    <td><center>relu</center></td>\n",
        "  </tr>\n",
        "   <tr>\n",
        "    <td><center>dropout</center></td>\n",
        "    <td><center>0.6</center></td>\n",
        "  <tr>\n",
        "    <tr>\n",
        "    <td><center>⚡ precision</center></td>\n",
        "    <td><center>16</center></td>\n",
        "  </tr>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>⚡ use_POS</center></td>\n",
        "    <td><center>True</center></td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td><center>⚡ use_lemmas</center></td>\n",
        "    <td><center>False</center></td>\n",
        "  </tr>\n",
        " \n",
        "\n",
        "</table>\n",
        "<br>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine vs Coarse-grained"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can do a lot of experiments and comparative analysis by using Fine/Coarse-grained WSD models to perform Fine/Coarse-grained WSD tasks. \n",
        "Let's see what we can do:\n",
        "*   Test all my models on *fine-grained* task and compare their perfomances with respect to the coarse one. \n",
        "*   Use a *fine-grained* model to predict *coarse-grained* senses. We can employ two strategies:\n",
        "    *   leverage a fine-grained model trained on fine-grained task and simply use it for coarse-grained inference.\n",
        "    *   instead, we can train the fine-grained model by giving a reward each time it predicts a fine-grained sense in the correct homonym set (we indeed change the loss function). At prediction time, we use it as in the previous case.\n",
        "*   Employ a *coarse-grained* model to *filter out* fine senses of a fine-grained model during inference.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> See how the hyperparameters ***coarse_loss_oriented***, ***predict_coarse_with_fine*** and ***predict_fine_with_coarse_filter*** are used to make all the experiments explained above happen in \"*predict*\" and \"*loss_function*\" methods in *WSD_Model* class."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **All you need is Gloss!**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As suggested by TAs, I decided to employ synsets definitions (hence **glosses**) in my pipeline. ***How?*** <br> <br> Following the idea of **GlossBert**. Basically, for each word to disambiguate we construct the so called *context-gloss* pairs. The sentence containing the target word is denoted as context sentence. For each target word, we extract the glosses of all possible senses of the target word in WordNet to obtain the gloss sentence. The input will be ( *[CLS] context_sentence [SEP] gloss [SEP]* ). <br> The bert-encoded target word is then fed into a binary classification layer (label ∈ {yes, no}). We want the model to learn if the relative gloss is the one which defines the target word. <br> Therefore, each target word has a set of context-gloss pair training instances (number of candidates). When testing, we output the probability of each context-gloss pair to be *yes* and choose the sense corresponding to the highest probability as prediction.\n",
        "\n",
        "> The glosses are already present in our dataset and they can be easily extracted. Considering the fact that the glosses are relative to fine-grained senses and not to coarse-grained ones, we can use two approaches when the task is coarse-grained oriented: <br> **1)** train a GlossBert ***fine-grained*** system and then trace back coarse-grained senses at inference time <br> (we can also try this method directly for the fine-grained task!); <br> **2)** directly train a GlossBert ***coarse-grained*** model by *concatenating* in *context-gloss* pairs the fine-grained glosses concerning the specific coarse-grained sense. Unfortunately this strategy won't be thoroughly investigated, both for the difficulty to be applied because of the very long input sequence and for the unsatisfactory initial results. We'll focus on the first approach then."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Being 512 the maximum token length allowed by bert-like models, we need to be careful when dealing with long input sequences. When using the simple architecture with no glosses involved, this wasn't a problem at all (in fact the biggest input tokens length I was able to detect was only 321!). But with glosses involved, things change, especially when concatenating glosses in the coarse-grained case. I developed a strategy to address the edge cases! "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BaynqYLrbhu9"
      },
      "source": [
        "## Training ⛏"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_AzCKIquGvIC"
      },
      "source": [
        "> Throughout all the project I used ***WandB*** as logging tool."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Coarse-grained training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "hparams_tuning = False\n",
        "version_name = \"bert_base\"\n",
        "with wandb.init(entity=\"lavallone\", project=\"NLP\", name=version_name, mode=\"online\"):\n",
        "    set_seed(1999)\n",
        "    hparams = asdict(Hparams())\n",
        "    hparams[\"coarse_or_grained\"] = \"coarse\"\n",
        "    hparams[\"use_gloss\"] = False\n",
        "\n",
        "    if hparams[\"use_gloss\"]:\n",
        "        data = WSD_Gloss_DataModule(hparams)\n",
        "        model = WSD_Gloss_Model(hparams)\n",
        "    else:\n",
        "        data = WSD_DataModule(hparams)\n",
        "        model = WSD_Model(hparams)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    \n",
        "    train_model(data, model, experiment_name=version_name, patience=5, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Fine-grained training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "hparams_tuning = False\n",
        "version_name = \"BASELINE\"\n",
        "with wandb.init(entity=\"lavallone\", project=\"NLP\", name=version_name, mode=\"online\"):\n",
        "    set_seed(1999)\n",
        "    hparams = asdict(Hparams())\n",
        "    hparams[\"coarse_or_grained\"] = \"fine\"\n",
        "    hparams[\"use_gloss\"] = False\n",
        "    hparams[\"coarse_loss_oriented\"] = False # if I want to train the model with a modified loss\n",
        "\n",
        "    if hparams[\"use_gloss\"]:\n",
        "        data = WSD_Gloss_DataModule(hparams)\n",
        "        model = WSD_Gloss_Model(hparams)\n",
        "    else:\n",
        "        data = WSD_DataModule(hparams)\n",
        "        model = WSD_Model(hparams)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    \n",
        "    train_model(data, model, experiment_name=version_name, patience=5, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_SwNRcdPbjvg"
      },
      "source": [
        "### Hyperparameters tuning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q9sF5ldZO5g6"
      },
      "source": [
        "> *WandB* provides a very user-friendly and easy-to-use *automated hyperparameter search* tool called ***Sweep***. This is how I tuned my hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_pipeline(config=None):\n",
        "    hparams_tuning = False\n",
        "    version_name = \"BASELINE\"\n",
        "    with wandb.init(entity=\"lavallone\", project=\"NLP\", name=version_name, mode=\"online\", config=config):\n",
        "        seed = wandb.config.seed if hparams_tuning else 1999\n",
        "        set_seed(seed)\n",
        "        hparams = asdict(Hparams())\n",
        "        # when doing the hparams search, this is how each run we change them to search for the best combinations!\n",
        "        if hparams_tuning:\n",
        "            hparams[\"batch_size\"] = wandb.config.batch_size\n",
        "            hparams[\"dropout\"] = wandb.config.dropout\n",
        "            hparams[\"lr\"] = wandb.config.lr\n",
        "            hparams[\"hidden_dim\"] = wandb.config.hidden_dim\n",
        "\n",
        "        data = WSD_DataModule(hparams)\n",
        "        model = WSD_Model(hparams)\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        model.to(device)\n",
        "        \n",
        "        train_model(data, model, experiment_name=version_name, patience=5, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCAV15D1bl14"
      },
      "outputs": [],
      "source": [
        "wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "sweep_config = {'method': 'random',\n",
        "                'metric': {'goal': 'maximize', 'name': 'val_micro_f1', 'target' : 0.89},\n",
        "                'parameters': {\n",
        "                                'batch_size': {'values': [64, 128, 256, 512]},\n",
        "                                'dropout': {'distribution': 'uniform', 'min': 0.3, 'max': 0.5},\n",
        "                                'lr': {'distribution': 'uniform', 'min': 1e-5, 'max': 1e-2},\n",
        "                                'hidden_dim': {'distribution': 'int_uniform', 'min': 200, 'max': 600},\n",
        "                            }\n",
        "               }\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project=\"NLP\", entity=\"lavallone\")\n",
        "wandb.agent(sweep_id, function=training_pipeline, count=20)\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ue4txjgQRtGs"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SC-4QOv7LMm2"
      },
      "source": [
        "> I also implemented an *evaluation pipeline* to avoid to execute each time the provided *test.sh* shell script. In fact it uses *docker* 🐳, which consumes a lot of storage space each run. Of course I'll use it for the final evaluation of my best model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Coarse-grained task**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wRe7SVyXMMJ_",
        "outputId": "1dddadae-d286-4b44-c048-578f22e5af02"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "best_coarse_ckpt = \"../../model/checkpoints/bert_base_pos_relu_4e-5-epoch=16-val_micro_f1=0.9416.ckpt\"\n",
        "use_gloss = False\n",
        "\n",
        "if use_gloss:\n",
        "    model = WSD_Gloss_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    data = WSD_Gloss_DataModule(model.hparams)\n",
        "else:\n",
        "    model = WSD_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "classification_report = evaluation_pipeline(model, data, use_gloss=use_gloss)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Fine-grained task**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TEST\n",
        "best_fine_ckpt = \"../../model/checkpoints/BASELINE_lemmas-epoch=24-val_micro_f1=0.9388.ckpt\"\n",
        "use_gloss = False\n",
        "\n",
        "if use_gloss:\n",
        "    model = WSD_Gloss_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    data = WSD_Gloss_DataModule(model.hparams)\n",
        "else:\n",
        "    model = WSD_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "classification_report = evaluation_pipeline(model, data, use_gloss=use_gloss)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Coarse-grained task - *from fine-grained predictions***\n",
        "> We can also test the model version trained using a modified loss function. Basically the system, in addition to the cross-entropy loss, is rewarded each time it predicts a fine-grained sense in the correct homonym set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TEST\n",
        "best_fine_ckpt = \"../../model/checkpoints/BASELINE_lemmas-epoch=24-val_micro_f1=0.9388.ckpt\"\n",
        "use_gloss = False\n",
        "\n",
        "if use_gloss:\n",
        "    model = WSD_Gloss_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    data = WSD_Gloss_DataModule(model.hparams)\n",
        "else:\n",
        "    model = WSD_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "model.hparams.predict_coarse_with_fine = True\n",
        "\n",
        "classification_report = evaluation_pipeline(model, data, use_gloss=use_gloss)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Fine-grained task - *through coarse-grained model filtering***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TEST\n",
        "best_fine_ckpt = \"../../model/checkpoints/BASELINE_lemmas-epoch=24-val_micro_f1=0.9388.ckpt\"\n",
        "best_coarse_ckpt = \"../../model/checkpoints/BASELINE_lemmas-epoch=24-val_micro_f1=0.9388.ckpt\"\n",
        "\n",
        "model = WSD_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams, coarse_filter_model=best_coarse_ckpt)\n",
        "data.setup()\n",
        "model.hparams.predict_fine_with_coarse_filter = True\n",
        "\n",
        "classification_report = evaluation_pipeline(model, data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nice further analysis can be done in two directions:\n",
        "*   evaluate performance on the **most difficult coarse-grained senses** (the very polysemous words!). I defined them in the *Data Analysis* section. It would be interesting to test all the models on this sense subset.\n",
        "*   test a prediction strategy which consists of always **selecting the first coarse-grained sense** (that's actually the most frequent one according to WordNet) and see what is the performance. Another cool thing to do would be to see if our models are really learning: a simple percentage of how many times the predicted sense is the first one or not will be computed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluation of the HARDEST coarse-grained senses!\n",
        "hparams = Hparams()\n",
        "use_gloss = False\n",
        "hard_words_to_disambiguate_dict = json.load(open(hparams.prefix_path+\"model/files/hard_words_to_disambiguate.json\", \"r\"))\n",
        "hard_words_candidates_set = [v for _,v in hard_words_to_disambiguate_dict.items()]\n",
        "coarse_sense2id = json.load(open(hparams.prefix_path+\"model/files/coarse_sense2id.json\", \"r\"))\n",
        "if not use_gloss: # we need to encode senses, otherwise is not needed\n",
        "    for sublist in hard_words_candidates_set:\n",
        "        for i in range(len(sublist)):\n",
        "            sublist[i] = coarse_sense2id[sublist[i]]\n",
        "hard_words_candidates_set = [set(v) for _,v in hard_words_to_disambiguate_dict.items()]\n",
        "\n",
        "# TEST\n",
        "best_ckpt = \"../../model/checkpoints/bert_base_pos_relu_4e-5-epoch=16-val_micro_f1=0.9416.ckpt\"\n",
        "use_gloss = False\n",
        "\n",
        "if use_gloss:\n",
        "    model = WSD_Gloss_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    data = WSD_Gloss_DataModule(model.hparams)\n",
        "else:\n",
        "    model = WSD_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "model.hparams.predict_coarse_with_fine = False # set to True when you want to predict coarse thanks to fine-grained senses\n",
        "\n",
        "classification_report = evaluation_pipeline(model, data, use_gloss=use_gloss, hard_words_candidates_set=hard_words_candidates_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 1/18 [00:03<01:04,  3.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[904, 473, 1776, 34, 1998]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 9/18 [00:07<00:06,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[904, 473, 1776, 34, 1998]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 18/18 [00:10<00:00,  1.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "| Micro F1 Score for test set if predicting the most common sense:  0.6796 |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# now I define the prediciton strategy which predicts the first sense (always!)\n",
        "from torchmetrics import F1Score\n",
        "hparams = asdict(Hparams())\n",
        "test_micro_f1 = F1Score(task=\"multiclass\", num_classes=hparams[\"num_senses\"], average=\"micro\")\n",
        "\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "preds_list, labels_list = [], []\n",
        "for batch in tqdm(data.test_dataloader()):\n",
        "    for cand_list in batch[\"candidates\"]:\n",
        "        if cand_list[0] == 904:\n",
        "            print(cand_list)\n",
        "        preds_list.append(cand_list[0]) # always the most frequent one\n",
        "    labels_list += batch[\"labels\"]\n",
        "    \n",
        "test_micro_f1 = test_micro_f1(torch.tensor(preds_list), torch.tensor(labels_list)).item()\n",
        "print()\n",
        "print(f\"| Micro F1 Score for test set if predicting the most common sense:  {round(test_micro_f1,4)} |\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 18/18 [00:59<00:00,  3.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Only the 19.55% of all predicted senses is the most frequent one!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# now I want to see how many times my model predict the most frequent sense and when not (only for my base models)\n",
        "\n",
        "# TEST\n",
        "best_ckpt = \"../../model/checkpoints/bert_base_pos_relu_4e-5-epoch=16-val_micro_f1=0.9416.ckpt\"\n",
        "model = WSD_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    tot_preds = 0\n",
        "    for batch in tqdm(data.test_dataloader()):\n",
        "        preds = model.predict(batch)\n",
        "        tot_preds += len(preds)\n",
        "\n",
        "    print(f\"Only the {round((model.first_sense_statistic/tot_preds)*100,2)}% of all predicted senses is the most frequent one!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
